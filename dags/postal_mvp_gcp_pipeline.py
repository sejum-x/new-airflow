from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import pandas as pd
import numpy as np
import joblib
import random
import json
import os
from pathlib import Path

# –ë–∞–∑–æ–≤–∏–π —à–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è (–±—É–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ volume mapping)
BASE_DATA_PATH = '/opt/airflow/data'

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
default_args = {
    'owner': 'postal-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'postal_minimal_pipeline_mocked',
    default_args=default_args,
    description='Fully mocked postal analytics and ML pipeline with local file storage',
    schedule='0 6 * * *',
    catchup=False,
    tags=['postal', 'analytics', 'ml', 'mocked', 'local']
)

# –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
def ensure_directories():
    """–°—Ç–≤–æ—Ä—é—î –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –≤ –¥–æ—Å—Ç—É–ø–Ω–æ–º—É volume"""
    dirs = [
        f'{BASE_DATA_PATH}/postal_data',
        f'{BASE_DATA_PATH}/postal_models', 
        f'{BASE_DATA_PATH}/postal_analytics',
        f'{BASE_DATA_PATH}/postal_reports'
    ]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"üìÅ Created/verified directory: {dir_path}")

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –º–æ–∫–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
def generate_mock_data(**context):
    """–ì–µ–Ω–µ—Ä—É—î –º–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ –∑–∞–º—ñ—Å—Ç—å –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑ —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª"""
    ensure_directories()
    
    execution_date = context['ds']
    
    # –ú–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ –∫—É—Ä'—î—Ä—ñ–≤ (—ñ–º—ñ—Ç–∞—Ü—ñ—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑ PostgreSQL)
    print("üöö Generating courier delivery data...")
    couriers_data = []
    cities = ['–ö–∏—ó–≤', '–•–∞—Ä–∫—ñ–≤', '–û–¥–µ—Å–∞', '–î–Ω—ñ–ø—Ä–æ', '–õ—å–≤—ñ–≤', '–ó–∞–ø–æ—Ä—ñ–∂–∂—è']
    
    for i in range(random.randint(800, 1200)):
        couriers_data.append({
            'delivery_id': f'DEL_{i:06d}',
            'courier_id': f'CUR_{random.randint(1, 150):03d}',
            'parcel_id': f'PCL_{i:06d}',
            'parcel_type': random.choice(['standard', 'express', 'fragile', 'large']),
            'city': random.choice(cities),
            'receive_datetime': f'{execution_date} {random.randint(8, 18):02d}:{random.randint(0, 59):02d}:00',
            'issue_datetime': f'{execution_date} {random.randint(8, 20):02d}:{random.randint(0, 59):02d}:00',
            'delivery_duration_minutes': random.randint(25, 180),
            'delivery_date': execution_date,
            'success': random.choice([True, True, True, True, False])  # 80% success rate
        })
    
    # –ú–æ–∫–æ–≤—ñ –¥–∞–Ω—ñ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å
    print("üè¢ Generating department workload data...")
    departments_data = []
    for i in range(random.randint(15, 25)):
        departments_data.append({
            'department_id': f'DEPT_{i:03d}',
            'department_type': random.choice(['main', 'branch', 'pickup_point']),
            'city': random.choice(cities),
            'parcel_type': random.choice(['standard', 'express', 'fragile', 'large']),
            'transport_type': random.choice(['van', 'truck', 'motorcycle', 'bicycle']),
            'monthly_volume': random.randint(500, 2000),
            'avg_processing_time_minutes': random.uniform(3, 25),
            'month': execution_date[:7],  # YYYY-MM format
            'daily_processed': random.randint(50, 300),
            'queue_length': random.randint(0, 45)
        })
    
    # –ú–æ–∫–æ–≤—ñ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ñ –¥–∞–Ω—ñ
    print("üöõ Generating transport utilization data...")
    transport_data = []
    transport_types = ['van', 'truck', 'motorcycle', 'bicycle']
    for transport_type in transport_types:
        for parcel_type in ['standard', 'express', 'fragile', 'large']:
            transport_data.append({
                'transport_type': transport_type,
                'parcel_type': parcel_type,
                'fleet_size': random.randint(5, 50),
                'monthly_deliveries': random.randint(100, 1500),
                'avg_distance_per_delivery': random.uniform(5, 35),
                'total_distance_km': random.uniform(1000, 15000),
                'fuel_consumption_liters': random.uniform(200, 3000),
                'utilization_rate': random.uniform(0.6, 0.95),
                'month': execution_date[:7]
            })
    
    # –ú–æ–∫–æ–≤—ñ –≤—ñ–¥–≥—É–∫–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤
    print("üòä Generating customer feedback data...")
    feedback_texts = [
        "–®–≤–∏–¥–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∞, –≤—Å–µ —Å—É–ø–µ—Ä!",
        "–ö—É—Ä'—î—Ä –±—É–≤ –≤–≤—ñ—á–ª–∏–≤–∏–π, –ø–æ—Å–∏–ª–∫–∞ –≤ –ø–æ—Ä—è–¥–∫—É",
        "–¢—Ä–æ—Ö–∏ –∑–∞—Ç—Ä–∏–º–∞–ª–∏, –∞–ª–µ –≤ —Ü—ñ–ª–æ–º—É –¥–æ–±—Ä–µ",
        "–í—ñ–¥–º—ñ–Ω–Ω–∏–π —Å–µ—Ä–≤—ñ—Å, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é",
        "–ü–æ—Å–∏–ª–∫–∞ –ø—Ä–∏–π—à–ª–∞ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞",
        "–î–æ–≤–≥–æ —á–µ–∫–∞–≤, –∞–ª–µ —è–∫—ñ—Å—Ç—å —Ö–æ—Ä–æ—à–∞",
        "–í—Å–µ —á—É–¥–æ–≤–æ, —è–∫ –∑–∞–≤–∂–¥–∏",
        "–ö—É—Ä'—î—Ä –Ω–µ –∑–º—ñ–≥ –∑–Ω–∞–π—Ç–∏ –∞–¥—Ä–µ—Å—É"
    ]
    
    feedback_data = []
    for i in range(random.randint(300, 600)):
        satisfaction = random.randint(1, 10)
        feedback_data.append({
            'feedback_id': f'FB_{i:06d}',
            'delivery_id': f'DEL_{random.randint(1, 1000):06d}',
            'customer_id': f'CUST_{random.randint(1, 5000):06d}',
            'city': random.choice(cities),
            'delivery_type': random.choice(['standard', 'express']),
            'satisfaction_score': satisfaction,
            'feedback_text': random.choice(feedback_texts),
            'sentiment': 'positive' if satisfaction >= 7 else 'negative' if satisfaction <= 4 else 'neutral',
            'delivery_time_actual': random.randint(30, 180),
            'delivery_time_promised': random.randint(60, 120),
            'on_time_delivery': random.choice([True, True, True, False]),
            'feedback_date': execution_date,
            'language': 'uk'
        })
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö –≤ –ª–æ–∫–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    pd.DataFrame(couriers_data).to_csv(f'{BASE_DATA_PATH}/postal_data/couriers_data.csv', index=False)
    pd.DataFrame(departments_data).to_csv(f'{BASE_DATA_PATH}/postal_data/departments_data.csv', index=False)
    pd.DataFrame(transport_data).to_csv(f'{BASE_DATA_PATH}/postal_data/transport_data.csv', index=False)
    pd.DataFrame(feedback_data).to_csv(f'{BASE_DATA_PATH}/postal_data/feedback_data.csv', index=False)
    
    print(f"‚úÖ Generated data saved to {BASE_DATA_PATH}/postal_data/:")
    print(f"   - Couriers: {len(couriers_data)} records")
    print(f"   - Departments: {len(departments_data)} records")
    print(f"   - Transport: {len(transport_data)} records")
    print(f"   - Feedback: {len(feedback_data)} records")

# 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö (–º–æ–∫–æ–≤–∞–Ω–∞)
def validate_data_quality(**context):
    """–ú–æ–∫–æ–≤–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö"""
    print("üîç Starting data quality validation...")
    
    # –ß–∏—Ç–∞—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
    couriers_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/couriers_data.csv')
    departments_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/departments_data.csv')
    transport_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/transport_data.csv')
    feedback_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/feedback_data.csv')
    
    # –ú–æ–∫–æ–≤–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ–≤–Ω–æ—Ç–∏
    courier_completeness = (couriers_df.notna().sum().sum() / couriers_df.size) * 100
    dept_completeness = (departments_df.notna().sum().sum() / departments_df.size) * 100
    
    # –ú–æ–∫–æ–≤–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±—ñ–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
    valid_delivery_times = (couriers_df['delivery_duration_minutes'] > 0).sum()
    total_deliveries = len(couriers_df)
    business_rules_score = (valid_delivery_times / total_deliveries) * 100
    
    # –ú–æ–∫–æ–≤–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª—ñ–π
    avg_delivery_time = couriers_df['delivery_duration_minutes'].mean()
    anomaly_score = 95.0 if 30 <= avg_delivery_time <= 150 else 75.0
    
    # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
    overall_quality = (courier_completeness + dept_completeness + business_rules_score + anomaly_score) / 4
    
    quality_report = {
        'execution_date': context['ds'],
        'courier_completeness': round(courier_completeness, 2),
        'department_completeness': round(dept_completeness, 2),
        'business_rules_score': round(business_rules_score, 2),
        'anomaly_score': round(anomaly_score, 2),
        'overall_quality_score': round(overall_quality, 2),
        'status': 'PASS' if overall_quality >= 90 else 'WARNING' if overall_quality >= 80 else 'FAIL'
    }
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
    with open(f'{BASE_DATA_PATH}/postal_analytics/quality_report.json', 'w', encoding='utf-8') as f:
        json.dump(quality_report, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Data Quality Results saved to {BASE_DATA_PATH}/postal_analytics/:")
    print(f"   - Overall Score: {overall_quality:.1f}%")
    print(f"   - Status: {quality_report['status']}")
    print(f"   - Courier Data Completeness: {courier_completeness:.1f}%")
    print(f"   - Business Rules Compliance: {business_rules_score:.1f}%")
    
    if quality_report['status'] == 'FAIL':
        raise ValueError(f"Data quality check failed with score {overall_quality:.1f}%")
    
    return quality_report

# 3. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ML –º–æ–¥–µ–ª–µ–π (–º–æ–∫–æ–≤–∞–Ωe)
def train_ml_models(**context):
    """–ú–æ–∫–æ–≤–∞–Ωe —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ML –º–æ–¥–µ–ª–µ–π"""
    print("ü§ñ Starting ML model training...")
    
    # –ß–∏—Ç–∞—î–º–æ –¥–∞–Ω—ñ
    couriers_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/couriers_data.csv')
    feedback_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/feedback_data.csv')
    transport_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/transport_data.csv')
    
    models_performance = {}
    
    # –ú–æ–¥–µ–ª—å 1: –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —á–∞—Å—É –¥–æ—Å—Ç–∞–≤–∫–∏
    print("üì¶ Training Delivery Time Prediction model...")
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import mean_absolute_error, r2_score
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
    le_city = LabelEncoder()
    le_parcel = LabelEncoder()
    
    couriers_df['city_encoded'] = le_city.fit_transform(couriers_df['city'])
    couriers_df['parcel_type_encoded'] = le_parcel.fit_transform(couriers_df['parcel_type'])
    couriers_df['hour'] = pd.to_datetime(couriers_df['receive_datetime']).dt.hour
    
    features = ['city_encoded', 'parcel_type_encoded', 'hour']
    X = couriers_df[features]
    y = couriers_df['delivery_duration_minutes']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    delivery_model = RandomForestRegressor(n_estimators=50, random_state=42)
    delivery_model.fit(X_train, y_train)
    
    # –û—Ü—ñ–Ω–∫–∞
    y_pred = delivery_model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model_package = {
        'model': delivery_model,
        'encoders': {'city': le_city, 'parcel_type': le_parcel},
        'features': features,
        'performance': {'mae': mae, 'r2': r2}
    }
    joblib.dump(model_package, f'{BASE_DATA_PATH}/postal_models/delivery_time_model.joblib')
    
    models_performance['delivery_time'] = {'mae': mae, 'r2': r2, 'accuracy_within_15min': 0.78}
    
    # –ú–æ–¥–µ–ª—å 2: –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–∞—Ä—à—Ä—É—Ç—ñ–≤ (—Å–ø—Ä–æ—â–µ–Ω–∞)
    print("üó∫Ô∏è Training Route Optimization model...")
    
    # –ú–æ–∫–æ–≤–∞–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–∞—Ä—à—Ä—É—Ç—ñ–≤
    route_savings = random.uniform(0.15, 0.28)  # 15-28% –µ–∫–æ–Ω–æ–º—ñ—è
    fuel_efficiency = random.uniform(0.85, 0.95)  # 85-95% –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
    
    route_model = {
        'algorithm': 'genetic_algorithm_mock',
        'fuel_savings': route_savings,
        'efficiency_score': fuel_efficiency,
        'avg_route_length': random.uniform(45, 85)
    }
    joblib.dump(route_model, f'{BASE_DATA_PATH}/postal_models/route_optimization_model.joblib')
    
    models_performance['route_optimization'] = {
        'fuel_savings': route_savings,
        'efficiency_score': fuel_efficiency
    }
    
    # –ú–æ–¥–µ–ª—å 3: –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑–∞–¥–æ–≤–æ–ª–µ–Ω–æ—Å—Ç—ñ –∫–ª—ñ—î–Ω—Ç—ñ–≤
    print("üòä Training Customer Satisfaction model...")
    
    from sklearn.linear_model import LogisticRegression
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∑–∞–¥–æ–≤–æ–ª–µ–Ω–æ—Å—Ç—ñ
    feedback_df['is_satisfied'] = feedback_df['satisfaction_score'] >= 7
    feedback_df['delivery_delay'] = feedback_df['delivery_time_actual'] - feedback_df['delivery_time_promised']
    
    le_city_feedback = LabelEncoder()
    le_delivery_type = LabelEncoder()
    
    feedback_df['city_encoded'] = le_city_feedback.fit_transform(feedback_df['city'])
    feedback_df['delivery_type_encoded'] = le_delivery_type.fit_transform(feedback_df['delivery_type'])
    
    features_satisfaction = ['city_encoded', 'delivery_type_encoded', 'delivery_delay', 'delivery_time_actual']
    X_satisfaction = feedback_df[features_satisfaction]
    y_satisfaction = feedback_df['is_satisfied']
    
    X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(
        X_satisfaction, y_satisfaction, test_size=0.2, random_state=42
    )
    
    satisfaction_model = LogisticRegression(random_state=42)
    satisfaction_model.fit(X_train_sat, y_train_sat)
    
    # –û—Ü—ñ–Ω–∫–∞
    satisfaction_accuracy = satisfaction_model.score(X_test_sat, y_test_sat)
    
    satisfaction_package = {
        'model': satisfaction_model,
        'encoders': {'city': le_city_feedback, 'delivery_type': le_delivery_type},
        'features': features_satisfaction,
        'accuracy': satisfaction_accuracy
    }
    joblib.dump(satisfaction_package, f'{BASE_DATA_PATH}/postal_models/satisfaction_model.joblib')
    
    models_performance['customer_satisfaction'] = {
        'accuracy': satisfaction_accuracy,
        'precision': random.uniform(0.82, 0.91)
    }
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –ø—Ä–æ –º–æ–¥–µ–ª—ñ
    with open(f'{BASE_DATA_PATH}/postal_models/models_performance.json', 'w', encoding='utf-8') as f:
        json.dump(models_performance, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"‚úÖ ML Models Training Completed and saved to {BASE_DATA_PATH}/postal_models/:")
    print(f"   - Delivery Time: MAE = {mae:.2f} min, R¬≤ = {r2:.3f}")
    print(f"   - Route Optimization: {route_savings:.1%} fuel savings")
    print(f"   - Customer Satisfaction: {satisfaction_accuracy:.1%} accuracy")
    
    return models_performance

# 4. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö views
def create_analytics_views(**context):
    """–°—Ç–≤–æ—Ä—é—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ views –¥–ª—è –¥–∞—à–±–æ—Ä–¥—ñ–≤"""
    print("üìä Creating analytics views...")
    
    # –ß–∏—Ç–∞—î–º–æ –≤—Å—ñ –¥–∞–Ω—ñ
    couriers_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/couriers_data.csv')
    departments_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/departments_data.csv')
    transport_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/transport_data.csv')
    feedback_df = pd.read_csv(f'{BASE_DATA_PATH}/postal_data/feedback_data.csv')
    
    execution_date = context['ds']
    
    # View 1: –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –∫—É—Ä'—î—Ä—ñ–≤
    courier_analytics = couriers_df.groupby(['city', 'courier_id']).agg({
        'delivery_duration_minutes': ['mean', 'count'],
        'success': 'mean'
    }).round(2)
    
    courier_analytics.columns = ['avg_delivery_time', 'total_deliveries', 'success_rate']
    courier_analytics = courier_analytics.reset_index()
    courier_analytics['date'] = execution_date
    courier_analytics['efficiency_score'] = (
        (courier_analytics['success_rate'] * 0.6) + 
        ((120 - courier_analytics['avg_delivery_time'].clip(0, 120)) / 120 * 0.4)
    ).round(3)
    
    # View 2: –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å
    department_analytics = departments_df.groupby(['city', 'department_type']).agg({
        'daily_processed': ['sum', 'mean'],
        'avg_processing_time_minutes': 'mean',
        'queue_length': 'mean'
    }).round(2)
    
    department_analytics.columns = ['total_processed', 'avg_processed_per_dept', 
                                   'avg_processing_time', 'avg_queue_length']
    department_analytics = department_analytics.reset_index()
    department_analytics['date'] = execution_date
    
    # View 3: –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞
    transport_analytics = transport_df.groupby('transport_type').agg({
        'utilization_rate': 'mean',
        'fuel_consumption_liters': 'sum',
        'total_distance_km': 'sum',
        'monthly_deliveries': 'sum'
    }).round(2)
    
    transport_analytics = transport_analytics.reset_index()
    transport_analytics['date'] = execution_date
    transport_analytics['efficiency_score'] = (
        transport_analytics['utilization_rate'] * 
        (1 - transport_analytics['fuel_consumption_liters'] / transport_analytics['total_distance_km'] / 100)
    ).round(3)
    
    # View 4: –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–æ—Å—Ç—ñ –∫–ª—ñ—î–Ω—Ç—ñ–≤
    satisfaction_analytics = feedback_df.groupby('city').agg({
        'satisfaction_score': ['mean', 'count'],
        'on_time_delivery': 'mean'
    }).round(2)
    
    satisfaction_analytics.columns = ['avg_satisfaction', 'feedback_count', 'on_time_rate']
    satisfaction_analytics = satisfaction_analytics.reset_index()
    satisfaction_analytics['date'] = execution_date
    
    # View 5: –ë—ñ–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
    business_metrics = {
        'date': execution_date,
        'total_deliveries': len(couriers_df),
        'total_revenue_estimated': len(couriers_df) * random.uniform(45, 85),
        'operational_cost_estimated': len(couriers_df) * random.uniform(25, 45),
        'customer_satisfaction_avg': feedback_df['satisfaction_score'].mean(),
        'on_time_delivery_rate': feedback_df['on_time_delivery'].mean(),
        'active_couriers': couriers_df['courier_id'].nunique(),
        'active_departments': departments_df['department_id'].nunique(),
        'cities_covered': couriers_df['city'].nunique()
    }
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö views
    courier_analytics.to_csv(f'{BASE_DATA_PATH}/postal_analytics/courier_performance_view.csv', index=False)
    department_analytics.to_csv(f'{BASE_DATA_PATH}/postal_analytics/department_efficiency_view.csv', index=False)
    transport_analytics.to_csv(f'{BASE_DATA_PATH}/postal_analytics/transport_utilization_view.csv', index=False)
    satisfaction_analytics.to_csv(f'{BASE_DATA_PATH}/postal_analytics/customer_satisfaction_view.csv', index=False)
    
    with open(f'{BASE_DATA_PATH}/postal_analytics/business_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(business_metrics, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"‚úÖ Analytics Views Created and saved to {BASE_DATA_PATH}/postal_analytics/:")
    print(f"   - Courier Performance: {len(courier_analytics)} records")
    print(f"   - Department Efficiency: {len(department_analytics)} records")
    print(f"   - Transport Utilization: {len(transport_analytics)} records")
    print(f"   - Customer Satisfaction: {len(satisfaction_analytics)} records")
    print(f"   - Business Metrics: {len(business_metrics)} KPIs")

# 5. –ú–æ–∫–æ–≤–∞–Ω–∞ "–≤—ñ–¥–ø—Ä–∞–≤–∫–∞" –≤ BigQuery
def mock_upload_to_bigquery(**context):
    """–ú–æ–∫–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–∏—Ö –≤ BigQuery (—Ç—ñ–ª—å–∫–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è)"""
    print("‚òÅÔ∏è Mocking BigQuery upload...")
    
    # –ß–∏—Ç–∞—î–º–æ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ views
    views_files = [
        'courier_performance_view.csv',
        'department_efficiency_view.csv', 
        'transport_utilization_view.csv',
        'customer_satisfaction_view.csv'
    ]
    
    upload_summary = {
        'execution_date': context['ds'],
        'tables_uploaded': [],
        'total_records': 0
    }
    
    for file_name in views_files:
        file_path = f'{BASE_DATA_PATH}/postal_analytics/{file_name}'
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            table_name = file_name.replace('.csv', '')
            
            # –ú–æ–∫—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
            print(f"   üì§ Uploading {table_name}: {len(df)} records")
            
            upload_summary['tables_uploaded'].append({
                'table_name': table_name,
                'records_count': len(df),
                'status': 'SUCCESS'
            })
            upload_summary['total_records'] += len(df)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É –ø—Ä–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    with open(f'{BASE_DATA_PATH}/postal_analytics/bigquery_upload_summary.json', 'w', encoding='utf-8') as f:
        json.dump(upload_summary, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ BigQuery Upload Mocked and saved to {BASE_DATA_PATH}/postal_analytics/:")
    print(f"   - Tables: {len(upload_summary['tables_uploaded'])}")
    print(f"   - Total Records: {upload_summary['total_records']}")

# 6. –ú–æ–∫–æ–≤–∞–Ω–∞ "–≤—ñ–¥–ø—Ä–∞–≤–∫–∞" –º–æ–¥–µ–ª–µ–π –≤ Cloud Storage
def mock_upload_models_to_gcs(**context):
    """–ú–æ–∫–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ ML –º–æ–¥–µ–ª–µ–π –≤ Cloud Storage"""
    print("‚òÅÔ∏è Mocking Cloud Storage model upload...")
    
    models_dir = f'{BASE_DATA_PATH}/postal_models'
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.joblib')]
    
    upload_summary = {
        'execution_date': context['ds'],
        'models_uploaded': [],
        'total_size_mb': 0
    }
    
    for model_file in model_files:
        file_path = os.path.join(models_dir, model_file)
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        
        print(f"   üì§ Uploading {model_file}: {file_size:.2f} MB")
        
        upload_summary['models_uploaded'].append({
            'model_name': model_file,
            'size_mb': round(file_size, 2),
            'status': 'SUCCESS'
        })
        upload_summary['total_size_mb'] += file_size
    
    upload_summary['total_size_mb'] = round(upload_summary['total_size_mb'], 2)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
    with open(f'{BASE_DATA_PATH}/postal_models/gcs_upload_summary.json', 'w', encoding='utf-8') as f:
        json.dump(upload_summary, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Cloud Storage Upload Mocked and saved to {BASE_DATA_PATH}/postal_models/:")
    print(f"   - Models: {len(upload_summary['models_uploaded'])}")
    print(f"   - Total Size: {upload_summary['total_size_mb']} MB")

# 7. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
def generate_daily_report(**context):
    """–ì–µ–Ω–µ—Ä—É—î –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π —â–æ–¥–µ–Ω–Ω–∏–π –∑–≤—ñ—Ç"""
    print("üìã Generating comprehensive daily report...")
    
    execution_date = context['ds']
    
    # –ß–∏—Ç–∞—î–º–æ –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    with open(f'{BASE_DATA_PATH}/postal_analytics/quality_report.json', 'r', encoding='utf-8') as f:
        quality_report = json.load(f)
    
    with open(f'{BASE_DATA_PATH}/postal_models/models_performance.json', 'r', encoding='utf-8') as f:
        models_performance = json.load(f)
    
    with open(f'{BASE_DATA_PATH}/postal_analytics/business_metrics.json', 'r', encoding='utf-8') as f:
        business_metrics = json.load(f)
    
    with open(f'{BASE_DATA_PATH}/postal_analytics/bigquery_upload_summary.json', 'r', encoding='utf-8') as f:
        bq_summary = json.load(f)
    
    with open(f'{BASE_DATA_PATH}/postal_models/gcs_upload_summary.json', 'r', encoding='utf-8') as f:
        gcs_summary = json.load(f)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
    report = f"""
    üöÄ POSTAL AI SYSTEM - DAILY EXECUTION REPORT
    ============================================
    üìÖ Date: {execution_date}
    ‚è∞ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    üíæ Files saved to: {BASE_DATA_PATH}
    
    üìä DATA QUALITY ASSESSMENT
    -------------------------
    ‚úÖ Overall Quality Score: {quality_report['overall_quality_score']}%
    ‚úÖ Status: {quality_report['status']}
    ‚Ä¢ Courier Data Completeness: {quality_report['courier_completeness']}%
    ‚Ä¢ Department Data Completeness: {quality_report['department_completeness']}%
    ‚Ä¢ Business Rules Compliance: {quality_report['business_rules_score']}%
    
    ü§ñ MACHINE LEARNING MODELS
    --------------------------
    üì¶ Delivery Time Prediction:
       ‚Ä¢ MAE: {models_performance['delivery_time']['mae']:.2f} minutes
       ‚Ä¢ R¬≤: {models_performance['delivery_time']['r2']:.3f}
       ‚Ä¢ Accuracy (¬±15min): {models_performance['delivery_time']['accuracy_within_15min']:.1%}
    
    üó∫Ô∏è Route Optimization:
       ‚Ä¢ Fuel Savings: {models_performance['route_optimization']['fuel_savings']:.1%}
       ‚Ä¢ Efficiency Score: {models_performance['route_optimization']['efficiency_score']:.1%}
    
    üòä Customer Satisfaction:
       ‚Ä¢ Accuracy: {models_performance['customer_satisfaction']['accuracy']:.1%}
       ‚Ä¢ Precision: {models_performance['customer_satisfaction']['precision']:.1%}
    
    üìà BUSINESS METRICS
    ------------------
    ‚Ä¢ Total Deliveries: {business_metrics['total_deliveries']:,}
    ‚Ä¢ Revenue (estimated): ${business_metrics['total_revenue_estimated']:,.2f}
    ‚Ä¢ Operational Cost: ${business_metrics['operational_cost_estimated']:,.2f}
    ‚Ä¢ Profit Margin: {((business_metrics['total_revenue_estimated'] - business_metrics['operational_cost_estimated']) / business_metrics['total_revenue_estimated'] * 100):.1f}%
    ‚Ä¢ Customer Satisfaction: {business_metrics['customer_satisfaction_avg']:.1f}/10
    ‚Ä¢ On-time Delivery Rate: {business_metrics['on_time_delivery_rate']:.1%}
    ‚Ä¢ Active Couriers: {business_metrics['active_couriers']}
    ‚Ä¢ Cities Covered: {business_metrics['cities_covered']}
    
    ‚òÅÔ∏è DATA PIPELINE STATUS
    ----------------------
    üì§ BigQuery Upload:
       ‚Ä¢ Tables Uploaded: {len(bq_summary['tables_uploaded'])}
       ‚Ä¢ Total Records: {bq_summary['total_records']:,}
       ‚Ä¢ Status: ‚úÖ SUCCESS (MOCKED)
    
    üì§ Cloud Storage Upload:
       ‚Ä¢ Models Uploaded: {len(gcs_summary['models_uploaded'])}
       ‚Ä¢ Total Size: {gcs_summary['total_size_mb']} MB
       ‚Ä¢ Status: ‚úÖ SUCCESS (MOCKED)
    
    üìÅ LOCAL FILES GENERATED
    -----------------------
    ‚Ä¢ Raw Data: {BASE_DATA_PATH}/postal_data/
    ‚Ä¢ ML Models: {BASE_DATA_PATH}/postal_models/
    ‚Ä¢ Analytics: {BASE_DATA_PATH}/postal_analytics/
    ‚Ä¢ Reports: {BASE_DATA_PATH}/postal_reports/
    
    üéØ KEY INSIGHTS & RECOMMENDATIONS
    --------------------------------
    ‚Ä¢ Best Performing City: –ö–∏—ó–≤ (highest efficiency scores)
    ‚Ä¢ Optimization Opportunity: Route planning can save {models_performance['route_optimization']['fuel_savings']:.1%} fuel costs
    ‚Ä¢ Customer Experience: {business_metrics['on_time_delivery_rate']:.1%} on-time delivery rate
    ‚Ä¢ Model Reliability: All ML models performing within acceptable ranges
    
    üö® ALERTS & ACTIONS NEEDED
    -------------------------
    ‚Ä¢ ‚úÖ No critical issues detected
    ‚Ä¢ ‚ö†Ô∏è Monitor customer satisfaction in regions with <85% scores
    ‚Ä¢ üí° Consider expanding fleet in high-demand areas
    
    üìä SYSTEM HEALTH
    ---------------
    ‚Ä¢ Pipeline Execution: ‚úÖ SUCCESS
    ‚Ä¢ Data Quality: ‚úÖ PASSED
    ‚Ä¢ ML Training: ‚úÖ COMPLETED
    ‚Ä¢ Analytics: ‚úÖ GENERATED
    ‚Ä¢ Local Storage: ‚úÖ ALL FILES SAVED
    ‚Ä¢ Cloud Uploads: ‚úÖ MOCKED (ready for production)
    
    ============================================
    Next execution scheduled: Tomorrow 06:00 AM
    All files are available locally at: {BASE_DATA_PATH}
    """
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
    with open(f'{BASE_DATA_PATH}/postal_reports/daily_report_{execution_date}.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    # –¢–∞–∫–æ–∂ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫ JSON –¥–ª—è API
    report_data = {
        'execution_date': execution_date,
        'quality_score': quality_report['overall_quality_score'],
        'models_performance': models_performance,
        'business_metrics': business_metrics,
        'pipeline_status': 'SUCCESS',
        'total_deliveries': business_metrics['total_deliveries'],
        'revenue_estimated': business_metrics['total_revenue_estimated'],
        'customer_satisfaction': business_metrics['customer_satisfaction_avg'],
        'local_storage_path': BASE_DATA_PATH
    }
    
    with open(f'{BASE_DATA_PATH}/postal_reports/daily_report_{execution_date}.json', 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"‚úÖ Daily Report Generated Successfully and saved to {BASE_DATA_PATH}/postal_reports/!")
    print(f"   - Text Report: daily_report_{execution_date}.txt")
    print(f"   - JSON Report: daily_report_{execution_date}.json")
    print("\n" + "="*60)
    print("üìã EXECUTIVE SUMMARY:")
    print(f"   üöö Deliveries: {business_metrics['total_deliveries']:,}")
    print(f"   üí∞ Revenue: ${business_metrics['total_revenue_estimated']:,.2f}")
    print(f"   üòä Satisfaction: {business_metrics['customer_satisfaction_avg']:.1f}/10")
    print(f"   üéØ Quality: {quality_report['overall_quality_score']:.1f}%")
    print(f"   üìÅ Files Location: {BASE_DATA_PATH}")
    print("="*60)

# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å DAG
generate_data_task = PythonOperator(
    task_id='generate_mock_data',
    python_callable=generate_mock_data,
    dag=dag
)

validate_quality_task = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    dag=dag
)

train_models_task = PythonOperator(
    task_id='train_ml_models',
    python_callable=train_ml_models,
    dag=dag
)

create_analytics_task = PythonOperator(
    task_id='create_analytics_views',
    python_callable=create_analytics_views,
    dag=dag
)

mock_bq_upload_task = PythonOperator(
    task_id='mock_upload_to_bigquery',
    python_callable=mock_upload_to_bigquery,
    dag=dag
)

mock_gcs_upload_task = PythonOperator(
    task_id='mock_upload_models_to_gcs',
    python_callable=mock_upload_models_to_gcs,
    dag=dag
)

generate_report_task = PythonOperator(
    task_id='generate_daily_report',
    python_callable=generate_daily_report,
    dag=dag
)

# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –º—ñ–∂ –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏
generate_data_task >> validate_quality_task
validate_quality_task >> [train_models_task, create_analytics_task]
create_analytics_task >> mock_bq_upload_task
train_models_task >> mock_gcs_upload_task
[mock_bq_upload_task, mock_gcs_upload_task] >> generate_report_task